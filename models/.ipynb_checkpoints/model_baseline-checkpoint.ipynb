{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cef1ab2-e537-4e95-9b4f-608b26e82a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.3.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 47746.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16000/16000 [00:02<00:00, 7114.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 72591.72it/s]\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ezequ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\ezequ\\AppData\\Local\\Temp\\ipykernel_63420\\3252075551.py:127: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16000/16000 [00:01<00:00, 8977.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16000/16000 [00:02<00:00, 7672.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 16000/16000 [00:21<00:00, 727.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 334375.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 501332.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 68267.85it/s]\n",
      "C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=TfidfVectorizer().\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 188>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Train LSA model\u001b[39;00m\n\u001b[0;32m    187\u001b[0m n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m--> 188\u001b[0m lsa \u001b[38;5;241m=\u001b[39m TruncatedSVD(n_components\u001b[38;5;241m=\u001b[39mn_components, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Reduce dimensionality\u001b[39;00m\n\u001b[0;32m    191\u001b[0m X_lsa \u001b[38;5;241m=\u001b[39m lsa\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:173\u001b[0m, in \u001b[0;36mTruncatedSVD.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit model on training data X.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m        Returns the transformer object.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:192\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit model to X and perform dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m        Reduced version of X. This will always be a dense array.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\base.py:557\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 557\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    558\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:753\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_2d:\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# If input is scalar raise error\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 753\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    754\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got scalar array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    755\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    756\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    757\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    758\u001b[0m         )\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=TfidfVectorizer().\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Import custom helper libraries\n",
    "import os, sys, re, csv, codecs\n",
    "import pickle\n",
    "\n",
    "# Maths modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import exp\n",
    "from numpy.core.fromnumeric import repeat, shape  # noqa: F401,W0611\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Viz modules\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Render for export\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "# import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)  \n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "#Sklearn modules\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay,PrecisionRecallDisplay,RocCurveDisplay,)\n",
    "from sklearn.metrics import (confusion_matrix, roc_auc_score, average_precision_score, classification_report)\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score)\n",
    "from sklearn.base import ClassifierMixin, is_classifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# System modules\n",
    "import random\n",
    "import contractions\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from unidecode import unidecode\n",
    "import logging\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "import gc\n",
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "# ML modules\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# NLTK modules\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Keras modules\n",
    "import keras\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, BatchNormalization, TimeDistributed, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.initializers import Constant\n",
    "# from keras.layers import (LSTM, Embedding, BatchNormalization, Dense, TimeDistributed, Dropout, Bidirectional, Flatten, GlobalMaxPool1D)\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# Tensoflow modules\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Gensim\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(r\"C:\\\\Users\\\\ezequ\\\\proyectos\\\\openclassrooms\\\\Projet_7\\\\data\\\\raw\\\\sentiment140_16000_tweets.csv\",\n",
    "                 names=[\"target\", \"text\"], encoding='latin-1')\n",
    "\n",
    "# Drop useless raw\n",
    "df = df.iloc[1: , :]\n",
    "\n",
    "#TEXT PREPROCESSING\n",
    "def text_cleaning(text, ponct, only_letters, numbers):\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "    ponctuation = \"[^!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\"\n",
    "    number = \"[^0-9]\"\n",
    "    letters = \"[^a-zA-Z ]\"\n",
    "    if ponct == 1:\n",
    "        text = re.sub(ponctuation, '', text)\n",
    "    if only_letters == 1:\n",
    "        text = re.sub(letters, '', text)\n",
    "    if numbers == 1:\n",
    "        text = re.sub(number, '', text)\n",
    "    return text\n",
    "\n",
    "# Let's put the text in lower case.\n",
    "df[\"new_text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "# Let's remove the punctuation.\n",
    "df['new_text'] = df.progress_apply(lambda x: text_cleaning(x['text'], 0, 1, 0),axis=1)\n",
    "\n",
    "# We can separate the text into word lists => each word unit is a tokens\n",
    "df['words'] = df.progress_apply(lambda x: word_tokenize(x['new_text']),axis=1)\n",
    "\n",
    "# Let's count the number of words per comment\n",
    "df['nb_words'] = df.progress_apply(lambda x: len(x['words']),axis=1)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "keep_words = []\n",
    "new_sw_nltk = [word for word in sw_nltk if word not in keep_words]\n",
    "new_sw_nltk.extend(['th','pm', 's', 'er', 'paris', 'rst', 'st', 'am', 'us'])\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(new_sw_nltk))\n",
    "cleaning = df['new_text'].str.replace(pat, '')\n",
    "df['new_words'] = cleaning.progress_apply(lambda x: nltk.word_tokenize(x))\n",
    "df['new_text'] = cleaning\n",
    "\n",
    "# The process of classifying words into their parts of speech and labeling \n",
    "# them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. \n",
    "\n",
    "def word_pos_tagger(list_words):\n",
    "    pos_tagged_text = nltk.pos_tag(list_words)\n",
    "    return pos_tagged_text\n",
    "\n",
    "all_reviews = df[\"new_text\"].str.cat(sep=' ')\n",
    "description_words = word_pos_tagger(nltk.word_tokenize(all_reviews))\n",
    "list_keep = []\n",
    "list_excl = ['IN', 'DT', 'CD', 'CC', 'RP', 'WDT', 'EX', 'MD', 'NNP', 'WDT', 'UH', 'WRB', \n",
    "'WP', 'WP$', 'PDT', 'PRP$', 'EX', 'POS', 'SYM', 'TO', 'NNPS']\n",
    "for word, tag in description_words:\n",
    "    if tag not in list_excl:\n",
    "        list_keep.append(tag)\n",
    "        \n",
    "df[\"text_tokens_pos_tagged\"] =  df[\"new_text\"].progress_apply(lambda x: nltk.word_tokenize(x))\n",
    "df[\"text_tokens_pos_tagged\"] =  df[\"text_tokens_pos_tagged\"].progress_apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "list_nouns = [\"NN\", \"NNS\"]\n",
    "df[\"words_subjects\"] =  df[\"text_tokens_pos_tagged\"].progress_apply(lambda x: [y for y, tag in x if tag in list_nouns])\n",
    "\n",
    "# The join() method takes all items in an iterable and joins them into one string.\n",
    "df[\"words_subjects\"] =  df[\"words_subjects\"].progress_apply(lambda x: \" \".join(x))\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    # w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    return lemmatizer.lemmatize(text)\n",
    "\n",
    "df[\"words_subjects_lem\"] = df[\"words_subjects\"].progress_apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "#label enconder\n",
    "le = LabelEncoder()\n",
    "le.fit(df['target'])\n",
    "df['target_encoded'] = le.transform(df['target'])\n",
    "\n",
    "# VECTORIZATION\n",
    "# Processed data path\n",
    "processed_data_path = os.path.join(\"..\", \"data\", \"processed\")\n",
    "vectorized_dataset_file_path = os.path.join(\n",
    "    processed_data_path, \"tfidf_spacy_dataset.pkl\"\n",
    ")\n",
    "vocabulary_file_path = os.path.join(processed_data_path, \"tfidf_spacy_vocabulary.pkl\")\n",
    "\n",
    "corpus = df[\"words_subjects_lem\"]\n",
    "# Define vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorize text\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get vocabulary\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "# Train LSA model\n",
    "n_components = 50\n",
    "lsa = TruncatedSVD(n_components=n_components, random_state=42).fit(X)\n",
    "\n",
    "# Reduce dimensionality\n",
    "X_lsa = lsa.transform(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "# set aside 20% of train and test data for evaluation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lsa, df[\"target_encoded\"], #df.target,\n",
    "    test_size=0.2,  stratify=df.target,shuffle = shuffle, random_state = 42)\n",
    "\n",
    "# Use the same function above for the validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "    test_size=0.1, random_state= 42) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Define model\n",
    "model = LogisticRegressionCV(random_state=42)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to disk\n",
    "pickle.dump(model, open('model.pkl', 'wb'))\n",
    "\n",
    "# # Loading the model to compare results\n",
    "# model = pickle.load(open('model.pkl', 'rb'))\n",
    "# print(model.predict([[2, 9, 6]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd07989-778f-4ea7-a56c-227161c1e063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp",
   "language": "python",
   "name": "yelp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
