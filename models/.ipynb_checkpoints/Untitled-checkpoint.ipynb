{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "560a0d8b-e212-4b0b-ae23-4ca0da34c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "#import  tweepy\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "import streamlit as st\n",
    "#from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeedff9a-932e-4701-b273-d3500475edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"##@I love the way you dance with me. So beautifull tonight, althoug you hate the music\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1d5683-4174-4afc-bb44-9c8cd9b53993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(raw_text):\n",
    "    \"\"\"\n",
    "    Function to convert a raw review to a string of words\n",
    "    The input is a single string (a raw text), and \n",
    "    the output is a single string (a preprocessed text)\n",
    "    \"\"\"\n",
    "    # 1. Remove HTML\n",
    "    raw_text = BeautifulSoup(raw_text,features=\"html.parser\").get_text() \n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", str(raw_text))\n",
    "    \n",
    "    # puncuation\n",
    "    text  = \"\".join([char for char in letters_only if char not in string.punctuation])\n",
    "    \n",
    "    # 3. Convert to lower case\n",
    "    words = text.lower().split() \n",
    "    \n",
    "    # 4. Tokens\n",
    "    # tokenizer_words = TweetTokenizer()\n",
    "    # tokens_sentences = [tokenizer_words.tokenize(t) for t in nltk.sent_tokenize(words)]\n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    # In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stop words to a set\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 4. Tokens\n",
    "    # word_tokenize(meaningful_words, language='english')\n",
    "    \n",
    "    # 6. Join the words back into one string separatSed by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9da5ad0-c88d-4b11-942c-98b220c6fd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##@I love the way you dance with me. So beautifull tonight, althoug you hate the music'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb54970e-5a8f-46ef-b5cb-98be0b664ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "    \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens if len(t) > 0]\n",
    "\n",
    "    return stems\n",
    "\n",
    "# words_stemmed = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")\n",
    "# print(words_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3769dd7d-3722-4d57-87e7-15fcdd8bb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"##@I love the way you dance with me. So beautifull tonight, althoug you hate the music\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb29836-cc51-4f39-b43b-492de66b511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = str([user_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72de7aec-d34d-4836-9b24-4841ec0e0a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = review_to_words(user_input)\n",
    "user_input = tokenize_and_stem(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67993b33-5d5e-4d0a-a6e5-9e89875165d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'way', 'danc', 'beautiful', 'tonight', 'althoug', 'hate', 'music']\n"
     ]
    }
   ],
   "source": [
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c755d6bb-f88e-4614-ac51-b78a8e1b07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def last_processing (user_input):\n",
    "    list_sentences_train = user_input\n",
    "\n",
    "    max_features = 20000\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list_sentences_train)\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    \n",
    "    word_tokenizer = Tokenizer()\n",
    "    word_tokenizer.fit_on_texts(list_sentences_train)\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(tokenizer.word_index)  + 1\n",
    "    \n",
    "    longest_train = max(list_sentences_train, key=lambda sentence: len(user_input))\n",
    "    length_long_sentence = len(word_tokenize(longest_train))\n",
    "    padded_sentences = pad_sequences(word_tokenizer.texts_to_sequences(list_sentences_train), length_long_sentence, padding='post')\n",
    "\n",
    "\n",
    "    maxlen = 300\n",
    "    X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    return X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0bb8760a-c5e9-4f47-ac80-3201b82ac6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hola = last_processing (user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e47cb5a2-41e3-491d-a165-bc5c543cc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec\n",
    "import gc\n",
    "\n",
    "def loadEmbeddingMatrix(typeToLoad):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        typeToLoad: word_embedding type\n",
    "    Returns:\n",
    "        Embedding_matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#load different embedding file from Kaggle depending on which embedding \n",
    "#matrix we are going to experiment with\n",
    "    model = word2vec.KeyedVectors.load_word2vec_format('C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin',binary=True,limit=100000)\n",
    "        #model = KeyedVectors.load_word2vec_format('C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin',binary=True,limit=100000)\n",
    "        #word2vec_model =  gensim.models.KeyedVectors.load_word2vec_format('C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin', encoding=\"utf8\", binary=True)\n",
    "    embed_size = 300\n",
    "    embeddings_index = dict()\n",
    "    for word in model.key_to_index:\n",
    "        embeddings_index[word] = model.word_vec(word)\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    gc.collect()\n",
    "#We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "#same statistics for the rest of our own random generated weights. \n",
    "    all_embs = np.hstack(list(embeddings_index.values()))\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    nb_words = len(tokenizer.word_index)\n",
    "#We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "#the size will be Number of Words in Vocab X Embedding Size\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    gc.collect()\n",
    "\n",
    "#With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "#our own dictionary and loaded pretrained embedding. \n",
    "    EMBEDDING_DIM = 300\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    embeddedCount = 0\n",
    "    for word, i in word_index.items():\n",
    "        i-=1\n",
    "        #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        #and store inside the embedding matrix that we will train later on.\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embeddedCount+=1\n",
    "    print('total embedded:',embeddedCount,'common words')\n",
    "\n",
    "    del(embeddings_index)\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    #finally, return the embedding matrix\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f32582f-916c-47ec-b238-b707e0bd4d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 17:22:22.074 INFO    gensim.models.keyedvectors: loading projection weights from C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin\n",
      "2022-07-15 17:22:22.978 INFO    gensim.utils: KeyedVectors lifecycle event {'msg': 'loaded (100000, 300) matrix of type float32 from C:\\\\Program Files (x86)\\\\GoogleNews-vectors-negative300\\\\GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2022-07-15T17:22:22.978758', 'gensim': '4.2.0', 'python': '3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:22:07) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'load_word2vec_format'}\n",
      "C:\\Users\\ezequ\\AppData\\Local\\Temp\\ipykernel_7312\\677076276.py:21: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embeddings_index[word] = model.word_vec(word)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 word vectors.\n",
      "total embedded: 6 common words\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_word2vec = loadEmbeddingMatrix('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e74facc-b86f-4609-987f-c7461729b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, BatchNormalization, TimeDistributed, Flatten\n",
    "\n",
    "\n",
    "maxlen=300\n",
    "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "x_word2vec = Embedding(vocab_size,\n",
    "          EMBEDDING_DIM,\n",
    "          weights=[embedding_matrix_word2vec],\n",
    "          input_length=length_long_sentence,\n",
    "          trainable=False)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b0c375c4-57da-4c1f-9375-9d1c0d6e5c8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://1f8b8e78-f8ab-4601-8127-31dfa5a8bb2c/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Loading model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_LSTM.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\saving\\pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     46\u001b[0m       \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(dest_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     47\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(archive\u001b[38;5;241m.\u001b[39mextractfile(name)\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m---> 48\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msave_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mrmtree(temp_dir)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:977\u001b[0m, in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    974\u001b[0m   loader \u001b[38;5;241m=\u001b[39m loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    975\u001b[0m                       ckpt_options, options, filters)\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 977\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    978\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    979\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    980\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    981\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    982\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loader, Loader):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://1f8b8e78-f8ab-4601-8127-31dfa5a8bb2c/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "#Loading model\n",
    "model = pickle.load(open(\"model_LSTM.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea584a58-8759-44cb-8784-77e7f17ad097",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://8b4f5729-ec28-4de4-9746-587143e66e5e/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the saved model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC://Users//ezequ//proyectos//openclassrooms//Projet_7//streamlit//model_LSTM.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\saving\\pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     46\u001b[0m       \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(dest_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     47\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(archive\u001b[38;5;241m.\u001b[39mextractfile(name)\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m---> 48\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msave_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mrmtree(temp_dir)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:977\u001b[0m, in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    974\u001b[0m   loader \u001b[38;5;241m=\u001b[39m loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    975\u001b[0m                       ckpt_options, options, filters)\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 977\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    978\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    979\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    980\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    981\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    982\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loader, Loader):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://8b4f5729-ec28-4de4-9746-587143e66e5e/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "with open(\"C://Users//ezequ//proyectos//openclassrooms//Projet_7//streamlit//model_LSTM.pkl\", 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4fb315b4-d3a4-48dd-a1e3-419273b94e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "path = './model.h5'\n",
    "loaded_model= tf.keras.models.load_model(path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5c8580e0-28f9-4144-b7a7-9b3a9bb5cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def sentiment_analysis(user_input):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # changing the input_data to numpy array\n",
    "    user_imput_as_numpy_array = np.asarray(user_input)\n",
    "    \n",
    "    # reshape the array as we are predicting for one instance\n",
    "    input_data_reshaped = user_imput_as_numpy_array.reshape(-1,1)\n",
    "    \n",
    "    \n",
    "    prediction = loaded_model.predict(input_data_reshaped) \n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bd5fcb50-6701-4008-aa53-af83931bc8fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 23:42:18.818 Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential/Cast' defined at (most recent call last):\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 707, in start\n      self.io_loop.start()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 502, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 491, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 398, in dispatch_shell\n      await result\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 722, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 382, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\ezequ\\AppData\\Local\\Temp\\ipykernel_7312\\589053769.py\", line 1, in <cell line: 1>\n      sentiment_analysis(user_input)\n    File \"C:\\Users\\ezequ\\AppData\\Local\\Temp\\ipykernel_7312\\566551348.py\", line 13, in sentiment_analysis\n      prediction = loaded_model.predict(input_data_reshaped)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1982, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step\n      outputs = model.predict_step(data)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n      return self(x, training=False)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 571, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 671, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential/Cast'\nCast string to float is not supported\n\t [[{{node sequential/Cast}}]] [Op:__inference_predict_function_141729]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentiment_analysis(user_input)\n",
      "Input \u001b[1;32mIn [89]\u001b[0m, in \u001b[0;36msentiment_analysis\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# reshape the array as we are predicting for one instance\u001b[39;00m\n\u001b[0;32m     10\u001b[0m input_data_reshaped \u001b[38;5;241m=\u001b[39m user_imput_as_numpy_array\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data_reshaped\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(prediction)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential/Cast' defined at (most recent call last):\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 707, in start\n      self.io_loop.start()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 502, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 491, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 398, in dispatch_shell\n      await result\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 722, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 382, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\ezequ\\AppData\\Local\\Temp\\ipykernel_7312\\589053769.py\", line 1, in <cell line: 1>\n      sentiment_analysis(user_input)\n    File \"C:\\Users\\ezequ\\AppData\\Local\\Temp\\ipykernel_7312\\566551348.py\", line 13, in sentiment_analysis\n      prediction = loaded_model.predict(input_data_reshaped)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1982, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step\n      outputs = model.predict_step(data)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n      return self(x, training=False)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 571, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 671, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential/Cast'\nCast string to float is not supported\n\t [[{{node sequential/Cast}}]] [Op:__inference_predict_function_141729]"
     ]
    }
   ],
   "source": [
    "sentiment_analysis(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461500d3-51fd-43da-9d96-17da9f6f46bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fe3a78d-0794-44ea-b77e-1f28fe47d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "list_sentences_train = user_input\n",
    "# list_sentences_test = df[\"words_subjects_st\"].values\n",
    "\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list_sentences_train)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f506ffa-51b1-4a21-8968-5c75c9d98566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4], [5], [6], [7], [8]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ae236f1-e035-41ee-a0c7-4a1b02ac8fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(list_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "641001fd-1ce9-47ff-8497-8c77616826f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index)  + 1\n",
    "#print(\"Vocabulary Size :\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4d90e75-2de7-488b-8da6-b066bc750bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "longest_train = max(list_sentences_train, key=lambda sentence: len(user_input))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "padded_sentences = pad_sequences(embed(list_sentences_train), length_long_sentence, padding='post')\n",
    "\n",
    "\n",
    "maxlen = 300\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "# X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de6db85a-cd54-46dc-958e-53e6efe37558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 2],\n",
       "       [0, 0, 0, ..., 0, 0, 3],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 6],\n",
       "       [0, 0, 0, ..., 0, 0, 7],\n",
       "       [0, 0, 0, ..., 0, 0, 8]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72b24b4f-20de-480d-b7e5-89938106b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization \n",
    "def stemming_text(meaningful_words):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    return stemmer.stem(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45280ace-c359-4624-bb1c-4a9416c01706",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = stemming_text(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cba396c-52c8-4dc1-a567-78206117ab92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##@i love the way you dance with me. so beautifull tonight, althoug you hate the music']\n"
     ]
    }
   ],
   "source": [
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d6136f1-a091-4079-bcca-2b0b7fdcef25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 82>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# st.write('FOR DEMO')\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# user_input = st.text_input(\"Write any tweet to check its sentiment\",\"EXAMPLE : this is my pet project and i love it.\")\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#Loading model\u001b[39;00m\n\u001b[0;32m     80\u001b[0m model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 82\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m##@I love the way you dance with me. So beautifull tonight, althoug you hate the music\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#cleansing and NLP part\u001b[39;00m\n\u001b[0;32m     85\u001b[0m user_input \u001b[38;5;241m=\u001b[39m review_to_words(user_input)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1159\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1158\u001b[0m     )\n\u001b[1;32m-> 1159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1201\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1198\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "# st.write(\"\"\"\n",
    "# # Predict Sentiment from Tweeters\n",
    "\n",
    "# An interactive Web app to perform Sentiment Analysis on Tweets, based on machine learning algorithm.\n",
    "\n",
    "# \"\"\")\n",
    "\n",
    "# #sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# #function that would clean the data\n",
    "# @st.cache\n",
    "def review_to_words(raw_text):\n",
    "    \"\"\"\n",
    "    Function to convert a raw review to a string of words\n",
    "    The input is a single string (a raw text), and \n",
    "    the output is a single string (a preprocessed text)\n",
    "    \"\"\"\n",
    "    # 1. Remove HTML\n",
    "    raw_text = BeautifulSoup(raw_text,features=\"html.parser\").get_text() \n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", str(raw_text))\n",
    "    \n",
    "    # puncuation\n",
    "    text  = \"\".join([char for char in letters_only if char not in string.punctuation])\n",
    "    \n",
    "    # 3. Convert to lower case\n",
    "    words_sm = text.lower().split() \n",
    "    \n",
    "    # 4. Tokens\n",
    "    tokenizer_words = TweetTokenizer()\n",
    "    tokens_sentences = [tokenizer_words.tokenize(t) for t in nltk.sent_tokenize([str(words_sm)])]\n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    # In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stop words to a set\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\")) \n",
    "    meaningful_words = [w for w in tokens_sentences if not w in stops]\n",
    "    \n",
    "    # 4. Tokens\n",
    "    # word_tokenize(meaningful_words, language='english')\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join(meaningful_words))\n",
    "\n",
    "# Lemmatization \n",
    "def stemming_text(meaningful_words):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    return stemmer.stem(meaningful_words)\n",
    "\n",
    "# st.write('FOR DEMO')\n",
    "# user_input = st.text_input(\"Write any tweet to check its sentiment\",\"EXAMPLE : this is my pet project and i love it.\")\n",
    "\n",
    "#Loading model\n",
    "model = pickle.load(open(\"model.pkl\", 'rb'))\n",
    "\n",
    "user_input = input(\"\")\n",
    "\n",
    "#cleansing and NLP part\n",
    "user_input = review_to_words(user_input)\n",
    "# user_input = stemming_text(user_input)\n",
    "\n",
    "user_input = [user_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f971dfa-dfdf-4db2-b783-4e015f4e1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define vectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # # Vectorize text\n",
    "# user_input = vectorizer.fit_transform(user_input)\n",
    "\n",
    "# # # Get vocabulary\n",
    "# vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a26681-e970-44d0-951b-afb9ec22eda3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TweetTokenizer, sent_tokenize\n\u001b[0;32m      3\u001b[0m tokenizer_words \u001b[38;5;241m=\u001b[39m TweetTokenizer()\n\u001b[0;32m      4\u001b[0m tokens_sentences \u001b[38;5;241m=\u001b[39m [tokenizer_words\u001b[38;5;241m.\u001b[39mtokenize(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m----> 5\u001b[0m nltk\u001b[38;5;241m.\u001b[39msent_tokenize(user_input)]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens_sentences)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matches \u001b[38;5;129;01mand\u001b[39;00m match\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "\n",
    "tokenizer_words = TweetTokenizer()\n",
    "tokens_sentences = [tokenizer_words.tokenize(t) for t in \n",
    "nltk.sent_tokenize(user_input)]\n",
    "print(tokens_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c6d02ab-e2fc-4eea-b814-39ad8eb90d68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meaningful_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(word_tokenize(meaningful_words))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'meaningful_words' is not defined"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8c2a5bf0-d8ba-40f6-a834-22f302c6d4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love way dance beautifull tonight althoug hate mus']\n"
     ]
    }
   ],
   "source": [
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7930e9bd-91d0-44d6-8659-c174fa0ad7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def sentiment_analysis(user_input):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # changing the input_data to numpy array\n",
    "    user_imput_as_numpy_array = np.asarray(user_input)\n",
    "    \n",
    "    # reshape the array as we are predicting for one instance\n",
    "    input_data_reshaped = user_imput_as_numpy_array.reshape(1,-1)\n",
    "    \n",
    "    \n",
    "    prediction = model.predict(input_data_reshaped ) \n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "628841f1-f3d9-4e04-bb69-db7cc56374cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love way dance beautifull tonight althoug hate mus']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc0a5599-fe37-43bf-8970-ec2823ed3041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ezequ\\miniconda3\\lib\\site-packages\\sklearn\\base.py:557: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  X = check_array(X, **check_params)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to convert array of bytes/strings into decimal numbers with dtype='numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:779\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 779\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'love way dance beautifull tonight althoug hate mus'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentiment_analysis(user_input)\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36msentiment_analysis\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# reshape the array as we are predicting for one instance\u001b[39;00m\n\u001b[0;32m     10\u001b[0m input_data_reshaped \u001b[38;5;241m=\u001b[39m user_imput_as_numpy_array\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data_reshaped\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(prediction)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:425\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m        Predicted class label per sample.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    427\u001b[0m         indices \u001b[38;5;241m=\u001b[39m (scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:407\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03mPredict confidence scores for samples.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    class would be predicted.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    405\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 407\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\base.py:557\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 557\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    558\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:781\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    779\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 781\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to convert array of bytes/strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    783\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minto decimal numbers with dtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    784\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    787\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    789\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to convert array of bytes/strings into decimal numbers with dtype='numeric'"
     ]
    }
   ],
   "source": [
    "sentiment_analysis(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92201f-f836-4d60-8dde-7a4d40015de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d3023-b9be-4e7a-b09a-366f440f17cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp",
   "language": "python",
   "name": "yelp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
