{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3da4593-7d42-4e2f-8746-5d1a1ce3e83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.3.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import custom helper libraries\n",
    "import os\n",
    "import sys\n",
    "import sys, os, re, csv, codecs\n",
    "import pickle\n",
    "\n",
    "# Maths modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import exp\n",
    "from numpy.core.fromnumeric import repeat, shape  # noqa: F401,W0611\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Viz modules\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Render for export\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "# import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)  \n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "#Sklearn modules\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay,PrecisionRecallDisplay,RocCurveDisplay,)\n",
    "from sklearn.metrics import (confusion_matrix, roc_auc_score, average_precision_score, classification_report)\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score)\n",
    "from sklearn.base import ClassifierMixin, is_classifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# System modules\n",
    "import random\n",
    "import contractions\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from unidecode import unidecode\n",
    "import logging\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "import gc\n",
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "# ML modules\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# NLTK modules\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Keras modules\n",
    "import keras\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, BatchNormalization, TimeDistributed, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.initializers import Constant\n",
    "# from keras.layers import (LSTM, Embedding, BatchNormalization, Dense, TimeDistributed, Dropout, Bidirectional, Flatten, GlobalMaxPool1D)\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# Tensoflow modules\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Gensim\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "384b2b4c-2755-4f3a-855c-d4e64995cbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 38379.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 58765.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16000/16000 [00:02<00:00, 7918.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 86251.64it/s]\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ezequ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\ezequ\\AppData\\Local\\Temp\\ipykernel_71540\\25484927.py:49: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:01<00:00, 10323.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16000/16000 [00:01<00:00, 9933.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 16000/16000 [00:18<00:00, 859.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 391282.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 640902.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 53117.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 19204\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV\n",
    "df = pd.read_csv(r\"C:\\\\Users\\\\ezequ\\\\proyectos\\\\openclassrooms\\\\Projet_7\\\\data\\\\raw\\\\sentiment140_16000_tweets.csv\",\n",
    "                 names=[\"target\", \"text\"], encoding='latin-1')\n",
    "\n",
    "# Drop useless raw\n",
    "df = df.iloc[1: , :]\n",
    "\n",
    "# user_input = \"##@I love the way you dance with me. So beautifull tonight, althoug you hate the music\"\n",
    "# user_input = str([user_input])\n",
    "\n",
    "#TEXT PREPROCESSING\n",
    "def text_cleaning(text, ponct, only_letters, numbers):\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "    ponctuation = \"[^!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\"\n",
    "    number = \"[^0-9]\"\n",
    "    letters = \"[^a-zA-Z ]\"\n",
    "    if ponct == 1:\n",
    "        text = re.sub(ponctuation, '', text)\n",
    "    if only_letters == 1:\n",
    "        text = re.sub(letters, '', text)\n",
    "    if numbers == 1:\n",
    "        text = re.sub(number, '', text)\n",
    "    return text\n",
    "# user_input = user_input.lower()\n",
    "\n",
    "# user_input = text_cleaning(user_input, 0,1,0)\n",
    "# Let's put the text in lower case.\n",
    "df[\"new_text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "df['new_text'] = df.progress_apply(lambda x: contractions.fix(x['text']),axis=1)\n",
    "\n",
    "# Let's remove the punctuation.\n",
    "df['new_text'] = df.progress_apply(lambda x: text_cleaning(x['text'], 0, 1, 0),axis=1)\n",
    "\n",
    "# # We can separate the text into word lists => each word unit is a tokens\n",
    "# words = word_tokenize(user_input)\n",
    "df['words'] = df.progress_apply(lambda x: word_tokenize(x['new_text']),axis=1)\n",
    "\n",
    "# # Let's count the number of words per comment\n",
    "df['nb_words'] = df.progress_apply(lambda x: len(x['words']),axis=1)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "keep_words = []\n",
    "new_sw_nltk = [word for word in sw_nltk if word not in keep_words]\n",
    "new_sw_nltk.extend(['th','pm', 's', 'er', 'paris', 'rst', 'st', 'am', 'us'])\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(new_sw_nltk))\n",
    "cleaning = df['new_text'].str.replace(pat, '')\n",
    "df['new_words'] = cleaning.progress_apply(lambda x: nltk.word_tokenize(x))\n",
    "df['new_text'] = cleaning\n",
    "\n",
    "# stops = set(stopwords.words(\"english\")) \n",
    "# meaningful_words = [w for w in words if not w in stops]\n",
    "# The process of classifying words into their parts of speech and labeling \n",
    "# them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. \n",
    "\n",
    "def word_pos_tagger(list_words):\n",
    "    pos_tagged_text = nltk.pos_tag(list_words)\n",
    "    return pos_tagged_text\n",
    "\n",
    "all_reviews = df[\"new_text\"].str.cat(sep=' ')\n",
    "description_words = word_pos_tagger(nltk.word_tokenize(all_reviews))\n",
    "list_keep = []\n",
    "list_excl = ['IN', 'DT', 'CD', 'CC', 'RP', 'WDT', 'EX', 'MD', 'NNP', 'WDT', 'UH', 'WRB', \n",
    "'WP', 'WP$', 'PDT', 'PRP$', 'EX', 'POS', 'SYM', 'TO', 'NNPS']\n",
    "for word, tag in description_words:\n",
    "    if tag not in list_excl:\n",
    "        list_keep.append(tag)\n",
    "        \n",
    "df[\"text_tokens_pos_tagged\"] =  df[\"new_text\"].progress_apply(lambda x: nltk.word_tokenize(x))\n",
    "df[\"text_tokens_pos_tagged\"] =  df[\"text_tokens_pos_tagged\"].progress_apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "list_nouns = [\"NN\", \"NNS\"]\n",
    "df[\"words_subjects\"] =  df[\"text_tokens_pos_tagged\"].progress_apply(lambda x: [y for y, tag in x if tag in list_nouns])\n",
    "\n",
    "# The join() method takes all items in an iterable and joins them into one string.\n",
    "df[\"words_subjects\"] =  df[\"words_subjects\"].progress_apply(lambda x: \" \".join(x))\n",
    "\n",
    "def stemming_text(word):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "df[\"words_subjects_st\"] = df[\"words_subjects\"].progress_apply(lambda x: stemming_text(x))\n",
    "\n",
    "# user_input = stemming_text(meaningful_words)# = user_input.progress_apply(lambda x: stemming_text(x))\n",
    "\n",
    "#label enconder\n",
    "le = LabelEncoder()\n",
    "le.fit(df['target'])\n",
    "df['target_encoded'] = le.transform(df['target'])\n",
    "\n",
    "#Vectorization\n",
    "list_classes = [\"target_encoded\"]\n",
    "y = df[list_classes].values\n",
    "embed_size=0\n",
    "\n",
    "list_sentences_train = df[\"words_subjects_st\"].values\n",
    "list_sentences_test = df[\"words_subjects_st\"].values\n",
    "\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list_sentences_train)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(list_sentences_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index)  + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)\n",
    "\n",
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "longest_train = max(list_sentences_train, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "padded_sentences = pad_sequences(embed(list_sentences_train), length_long_sentence, padding='post')\n",
    "\n",
    "\n",
    "maxlen = 300\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "196d0480-f86d-4e73-aba1-576c4ec5f9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     0,   720,    97],\n",
       "       [    0,     0,     0, ...,   295,   213,    29],\n",
       "       [    0,     0,     0, ...,     4,    59,   426],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   204, 19200, 19201],\n",
       "       [    0,     0,     0, ...,  3566,  1516,  4750],\n",
       "       [    0,     0,     0, ...,   299,    84, 19203]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d58d21d7-5ae7-4b4c-a3ac-8c8e1620561a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love the way you dance with me so beautifull tonight althoug you hate the music'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47a50c07-1c80-4eda-b276-a259ff35c3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'way', 'dance', 'beautifull', 'tonight', 'althoug', 'hate', 'music']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4646b7-b6cf-46f0-a82c-741c4bcd7cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d582cf5f-528a-43e1-be38-828f9a0d98be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.3.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 63662.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16000/16000 [00:01<00:00, 8747.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 96045.71it/s]\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ezequ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\ezequ\\AppData\\Local\\Temp\\ipykernel_71540\\3782614066.py:127: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:01<00:00, 11086.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:01<00:00, 11642.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 16000/16000 [00:19<00:00, 829.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 433167.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 552509.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 48830.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 19204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ezequ\\AppData\\Local\\Temp\\ipykernel_71540\\3782614066.py:217: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `word_vec` (Use get_vector instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 word vectors.\n",
      "total embedded: 5314 common words\n",
      "Epoch 1/10\n",
      "360/360 [==============================] - 163s 444ms/step - loss: 0.7059 - accuracy: 0.4970 - val_loss: 0.7027 - val_accuracy: 0.4977\n",
      "Epoch 2/10\n",
      "360/360 [==============================] - 156s 434ms/step - loss: 0.6943 - accuracy: 0.5253 - val_loss: 0.7343 - val_accuracy: 0.4961\n",
      "Epoch 3/10\n",
      "360/360 [==============================] - 152s 422ms/step - loss: 0.6795 - accuracy: 0.5703 - val_loss: 0.7407 - val_accuracy: 0.4836\n",
      "Epoch 4/10\n",
      "360/360 [==============================] - 143s 397ms/step - loss: 0.6192 - accuracy: 0.6639 - val_loss: 0.8161 - val_accuracy: 0.5227\n",
      "INFO:tensorflow:Assets written to: ram://f56f1571-b729-4b67-9cba-e95c1839ddf0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001EC65E96100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001EC62C042E0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# Import custom helper libraries\n",
    "import os\n",
    "import sys\n",
    "import sys, os, re, csv, codecs\n",
    "import pickle\n",
    "\n",
    "# Maths modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import exp\n",
    "from numpy.core.fromnumeric import repeat, shape  # noqa: F401,W0611\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Viz modules\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Render for export\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "# import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)  \n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "#Sklearn modules\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay,PrecisionRecallDisplay,RocCurveDisplay,)\n",
    "from sklearn.metrics import (confusion_matrix, roc_auc_score, average_precision_score, classification_report)\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score)\n",
    "from sklearn.base import ClassifierMixin, is_classifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# System modules\n",
    "import random\n",
    "import contractions\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from unidecode import unidecode\n",
    "import logging\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "import gc\n",
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "# ML modules\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# NLTK modules\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Keras modules\n",
    "import keras\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, BatchNormalization, TimeDistributed, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.initializers import Constant\n",
    "# from keras.layers import (LSTM, Embedding, BatchNormalization, Dense, TimeDistributed, Dropout, Bidirectional, Flatten, GlobalMaxPool1D)\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# Tensoflow modules\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Gensim\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(r\"C:\\\\Users\\\\ezequ\\\\proyectos\\\\openclassrooms\\\\Projet_7\\\\data\\\\raw\\\\sentiment140_16000_tweets.csv\",\n",
    "                 names=[\"target\", \"text\"], encoding='latin-1')\n",
    "\n",
    "# Drop useless raw\n",
    "df = df.iloc[1: , :]\n",
    "\n",
    "#TEXT PREPROCESSING\n",
    "def text_cleaning(text, ponct, only_letters, numbers):\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "    ponctuation = \"[^!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\"\n",
    "    number = \"[^0-9]\"\n",
    "    letters = \"[^a-zA-Z ]\"\n",
    "    if ponct == 1:\n",
    "        text = re.sub(ponctuation, '', text)\n",
    "    if only_letters == 1:\n",
    "        text = re.sub(letters, '', text)\n",
    "    if numbers == 1:\n",
    "        text = re.sub(number, '', text)\n",
    "    return text\n",
    "\n",
    "# Let's put the text in lower case.\n",
    "df[\"new_text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "# Let's remove the punctuation.\n",
    "df['new_text'] = df.progress_apply(lambda x: text_cleaning(x['text'], 0, 1, 0),axis=1)\n",
    "\n",
    "# We can separate the text into word lists => each word unit is a tokens\n",
    "df['words'] = df.progress_apply(lambda x: word_tokenize(x['new_text']),axis=1)\n",
    "\n",
    "# Let's count the number of words per comment\n",
    "df['nb_words'] = df.progress_apply(lambda x: len(x['words']),axis=1)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "sw_nltk = stopwords.words('english')\n",
    "keep_words = []\n",
    "new_sw_nltk = [word for word in sw_nltk if word not in keep_words]\n",
    "new_sw_nltk.extend(['th','pm', 's', 'er', 'paris', 'rst', 'st', 'am', 'us'])\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(new_sw_nltk))\n",
    "cleaning = df['new_text'].str.replace(pat, '')\n",
    "df['new_words'] = cleaning.progress_apply(lambda x: nltk.word_tokenize(x))\n",
    "df['new_text'] = cleaning\n",
    "\n",
    "# The process of classifying words into their parts of speech and labeling \n",
    "# them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. \n",
    "\n",
    "def word_pos_tagger(list_words):\n",
    "    pos_tagged_text = nltk.pos_tag(list_words)\n",
    "    return pos_tagged_text\n",
    "\n",
    "all_reviews = df[\"new_text\"].str.cat(sep=' ')\n",
    "description_words = word_pos_tagger(nltk.word_tokenize(all_reviews))\n",
    "list_keep = []\n",
    "list_excl = ['IN', 'DT', 'CD', 'CC', 'RP', 'WDT', 'EX', 'MD', 'NNP', 'WDT', 'UH', 'WRB', \n",
    "'WP', 'WP$', 'PDT', 'PRP$', 'EX', 'POS', 'SYM', 'TO', 'NNPS']\n",
    "for word, tag in description_words:\n",
    "    if tag not in list_excl:\n",
    "        list_keep.append(tag)\n",
    "        \n",
    "df[\"text_tokens_pos_tagged\"] =  df[\"new_text\"].progress_apply(lambda x: nltk.word_tokenize(x))\n",
    "df[\"text_tokens_pos_tagged\"] =  df[\"text_tokens_pos_tagged\"].progress_apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "list_nouns = [\"NN\", \"NNS\"]\n",
    "df[\"words_subjects\"] =  df[\"text_tokens_pos_tagged\"].progress_apply(lambda x: [y for y, tag in x if tag in list_nouns])\n",
    "\n",
    "# The join() method takes all items in an iterable and joins them into one string.\n",
    "df[\"words_subjects\"] =  df[\"words_subjects\"].progress_apply(lambda x: \" \".join(x))\n",
    "\n",
    "def stemming_text(word):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "df[\"words_subjects_st\"] = df[\"words_subjects\"].progress_apply(lambda x: stemming_text(x))\n",
    "\n",
    "#label enconder\n",
    "le = LabelEncoder()\n",
    "le.fit(df['target'])\n",
    "df['target_encoded'] = le.transform(df['target'])\n",
    "\n",
    "#Vectorization\n",
    "list_classes = [\"target_encoded\"]\n",
    "y = df[list_classes].values\n",
    "embed_size=0\n",
    "\n",
    "list_sentences_train = df[\"words_subjects_st\"].values\n",
    "list_sentences_test = df[\"words_subjects_st\"].values\n",
    "\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list_sentences_train)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(list_sentences_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index)  + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)\n",
    "\n",
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "longest_train = max(list_sentences_train, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "padded_sentences = pad_sequences(embed(list_sentences_train), length_long_sentence, padding='post')\n",
    "\n",
    "\n",
    "maxlen = 300\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "def loadEmbeddingMatrix(typeToLoad):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        typeToLoad: word_embedding type\n",
    "    Returns:\n",
    "        Embedding_matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#load different embedding file from Kaggle depending on which embedding \n",
    "#matrix we are going to experiment with\n",
    "    model = word2vec.KeyedVectors.load_word2vec_format('C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin',binary=True,limit=100000)\n",
    "        #model = KeyedVectors.load_word2vec_format('C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin',binary=True,limit=100000)\n",
    "        #word2vec_model =  gensim.models.KeyedVectors.load_word2vec_format('C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin', encoding=\"utf8\", binary=True)\n",
    "    embed_size = 300\n",
    "    embeddings_index = dict()\n",
    "    for word in model.key_to_index:\n",
    "        embeddings_index[word] = model.word_vec(word)\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    gc.collect()\n",
    "#We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "#same statistics for the rest of our own random generated weights. \n",
    "    all_embs = np.hstack(list(embeddings_index.values()))\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    nb_words = len(tokenizer.word_index)\n",
    "#We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "#the size will be Number of Words in Vocab X Embedding Size\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    gc.collect()\n",
    "\n",
    "#With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "#our own dictionary and loaded pretrained embedding. \n",
    "    EMBEDDING_DIM = 300\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    embeddedCount = 0\n",
    "    for word, i in word_index.items():\n",
    "        i-=1\n",
    "        #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        #and store inside the embedding matrix that we will train later on.\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embeddedCount+=1\n",
    "    print('total embedded:',embeddedCount,'common words')\n",
    "\n",
    "    del(embeddings_index)\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    #finally, return the embedding matrix\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# def loadEmbeddingMatrix(typeToLoad):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         typeToLoad: word_embedding type\n",
    "#     Returns:\n",
    "#         Embedding_matrix.\n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "# #load different embedding file from Kaggle depending on which embedding \n",
    "# #matrix we are going to experiment with\n",
    "#     if(typeToLoad==\"glove\"):\n",
    "#         EMBEDDING_FILE= ('C://Users//ezequ//proyectos//openclassrooms//Projet_7//data//raw//glove.6B.300d.txt')\n",
    "#         embed_size = 300\n",
    "#     elif(typeToLoad==\"word2vec\"):\n",
    "# #             w2v_model = Word2Vec.load('C://Users//ezequ//proyectos//openclassrooms//Projet_7//data//raw//GoogleNews-vectors-negative300.bin',binary=True,limit=100000)\n",
    "# #             model = w2v_model.wv\n",
    "#         model = word2vec.KeyedVectors.load_word2vec_format('C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin',binary=True,limit=100000)\n",
    "#         #model = KeyedVectors.load_word2vec_format('C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin',binary=True,limit=100000)\n",
    "#         #word2vec_model =  gensim.models.KeyedVectors.load_word2vec_format('C:\\Program Files (x86)\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin', encoding=\"utf8\", binary=True)\n",
    "#         embed_size = 300\n",
    "#     elif(typeToLoad==\"fasttext\"):\n",
    "#         EMBEDDING_FILE= ('C://Users//ezequ//proyectos//openclassrooms//Projet_7//data//raw//wiki-news-300d-1M.vec')\n",
    "#         embed_size = 300\n",
    "\n",
    "#     if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\n",
    "#         embeddings_index = dict()\n",
    "#         #Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "#         f = open(EMBEDDING_FILE, encoding=\"utf8\")\n",
    "#         for line in f:\n",
    "#             #split up line into an indexed array\n",
    "#             values = line.split()\n",
    "#             #first index is word\n",
    "#             word = values[0]\n",
    "#             #store the rest of the values in the array as a new array\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs #50 dimensions\n",
    "#         f.close()\n",
    "#         print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "#     else:\n",
    "#         embeddings_index = dict()\n",
    "#         for word in model.key_to_index:\n",
    "#             embeddings_index[word] = model.word_vec(word)\n",
    "#         print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#     gc.collect()\n",
    "# #We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "# #same statistics for the rest of our own random generated weights. \n",
    "#     all_embs = np.hstack(list(embeddings_index.values()))\n",
    "#     emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "#     nb_words = len(tokenizer.word_index)\n",
    "# #We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "# #the size will be Number of Words in Vocab X Embedding Size\n",
    "#     embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "#     gc.collect()\n",
    "\n",
    "# #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "# #our own dictionary and loaded pretrained embedding. \n",
    "#     EMBEDDING_DIM = 300\n",
    "#     embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "#     embeddedCount = 0\n",
    "#     for word, i in word_index.items():\n",
    "#         i-=1\n",
    "#         #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         #and store inside the embedding matrix that we will train later on.\n",
    "#         if embedding_vector is not None: \n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "#             embeddedCount+=1\n",
    "#     print('total embedded:',embeddedCount,'common words')\n",
    "\n",
    "#     del(embeddings_index)\n",
    "#     gc.collect()\n",
    "    \n",
    "\n",
    "#     #finally, return the embedding matrix\n",
    "#     return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix_word2vec = loadEmbeddingMatrix('word2vec')\n",
    "\n",
    "maxlen=300\n",
    "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "x_word2vec = Embedding(vocab_size,\n",
    "          EMBEDDING_DIM,\n",
    "          weights=[embedding_matrix_word2vec],\n",
    "          input_length=length_long_sentence,\n",
    "          trainable=False)(inp)\n",
    "\n",
    "# Split data into train and test sets\n",
    "# set aside 20% of train and test data for evaluation\n",
    "# Manually shuffle the data to avoind val_acc and loss_acc with insignificant values\n",
    "np.random.shuffle(X_t)\n",
    "np.random.shuffle(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y,\n",
    "    test_size=0.2, shuffle = shuffle, random_state = 42)\n",
    "\n",
    "# Use the same function above for the validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "    test_size=0.1, random_state= 42) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "#LSTM Model\n",
    "def lstm_1():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix_word2vec.shape[0], \n",
    "        output_dim=embedding_matrix_word2vec.shape[1], \n",
    "        weights = [embedding_matrix_word2vec], \n",
    "        input_length=maxlen\n",
    "    ))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(\n",
    "        length_long_sentence, \n",
    "        return_sequences = True, \n",
    "        recurrent_dropout=0.2\n",
    "    )))\n",
    "    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = lstm_1()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model.fit(X_train,y_train, batch_size=batch_size, epochs=epochs,  validation_data= [X_val, y_val], callbacks=[EarlyStopping(monitor=\"val_loss\", patience=3),], workers=4,use_multiprocessing=True,)\n",
    "\n",
    "# Save the model to disk\n",
    "pickle.dump(model, open('model_LSTM.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e23a864f-6769-4bad-ae30-6e17afaca689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.saved_model.load_options.LoadOptions at 0x1ec5a77d9c0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.saved_model.LoadOptions(\n",
    "    allow_partial_checkpoint=False,\n",
    "    experimental_io_device=None,\n",
    "    experimental_skip_checkpoint=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a12fc2ad-48fb-482d-8cb9-70d9b8bd115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://86af730e-0361-40cd-a680-2f2edbeeb9f3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://86af730e-0361-40cd-a680-2f2edbeeb9f3/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001EC65E96100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001EC62C042E0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e534be9a-3ee6-487a-9343-2a1dffdd75f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://bde06988-7895-4fb9-b076-c124796f5a5d/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the model from disk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\saving\\pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     46\u001b[0m       \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(dest_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     47\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(archive\u001b[38;5;241m.\u001b[39mextractfile(name)\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m---> 48\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msave_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mrmtree(temp_dir)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:977\u001b[0m, in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    974\u001b[0m   loader \u001b[38;5;241m=\u001b[39m loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    975\u001b[0m                       ckpt_options, options, filters)\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 977\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    978\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    979\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    980\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    981\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    982\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loader, Loader):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://bde06988-7895-4fb9-b076-c124796f5a5d/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8037ee-512c-4770-9d46-886503790860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "# Save the Tokenizer and the Model in the same file\n",
    "with open('model_and_tokenizer.pkl', 'wb') as file:\n",
    "  pickle.dump((tokenizer, model), file)\n",
    " \n",
    " \n",
    "# Load the Tokenizer and the Model\n",
    "with open('model_and_tokenizer.pkl', 'rb') as file:\n",
    "  tokenizer, model = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp",
   "language": "python",
   "name": "yelp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
