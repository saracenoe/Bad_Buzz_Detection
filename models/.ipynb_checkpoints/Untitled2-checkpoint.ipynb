{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81754d4-8b8d-4923-87e8-f2307b7d36a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 79>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite any tweet to check its sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXAMPLE : this is my pet project and i love it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m#Loading model\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# model = pickle.load(open(\"model_LSTM.pkl\", 'rb'))\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m[user_input]\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m#cleansing and NLP part\u001b[39;00m\n\u001b[0;32m     81\u001b[0m user_input \u001b[38;5;241m=\u001b[39m review_to_words(user_input)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "import streamlit as st\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import time\n",
    "import streamlit as st\n",
    "\n",
    "# st.write(\"\"\"\n",
    "# # Predict Sentiment from Tweeters\n",
    "\n",
    "# An interactive Web app to perform Sentiment Analysis on Tweets, based on machine learning algorithm.\n",
    "\n",
    "# \"\"\")\n",
    "\n",
    "#sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#function that would clean the data\n",
    "# @st.cache\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw text), and \n",
    "    # the output is a single string (a preprocessed text)\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review,features=\"html.parser\").get_text() \n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    \n",
    "    #puncuation\n",
    "    text  = \"\".join([char for char in  letters_only if char not in string.punctuation])\n",
    "    \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = text.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    \n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join(meaningful_words )) \n",
    "\n",
    "# Lemmatization \n",
    "def lemmatize_text(meaningful_words):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    # w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    return lemmatizer.lemmatize(meaningful_words)\n",
    "\n",
    "# st.write('FOR DEMO')\n",
    "# user_input = st.text_input(\"Write any tweet to check its sentiment\",\"EXAMPLE : this is my pet project and i love it.\")\n",
    "user_input = \"Write any tweet to check its sentiment\",\"EXAMPLE : this is my pet project and i love it.\"\n",
    "\n",
    "#Loading model\n",
    "# model = pickle.load(open(\"model_LSTM.pkl\", 'rb'))\n",
    "user_input = str([user_input])\n",
    "#cleansing and NLP part\n",
    "user_input = review_to_words(user_input)\n",
    "user_input = lemmatize_text(user_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bed72-8edc-48a8-ad7c-0e46e59fd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb4732-5003-4020-be76-29814a193d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yelp",
   "language": "python",
   "name": "yelp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
